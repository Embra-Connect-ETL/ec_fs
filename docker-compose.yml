services:
  ec_fs_api:
    container_name: ec_fs_api
    image: embraconnect/ec_fs_api:latest
    build:
      dockerfile: api.dockerfile
      context: .
    environment:
      - EC_FS_ENDPOINT=minio:9000
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - EC_FS_BUCKET=${EC_FS_BUCKET}
    depends_on:
      - cache
      - ec_scheduler
      - minio
      - mongo-db
      - spark
    links:
      - "cache"
      - "ec_scheduler"
      - "minio"
      - "mongo-db"
      - "spark"
    restart: always
    ports:
      - "3307:3307"
    networks:
      - ec_network

  ec_fs_ui:
    container_name: ec_fs_ui
    image: embraconnect/ec_fs_ui:latest
    build:
      dockerfile: ui.dockerfile
      context: .
    depends_on:
      - cache
      - ec_fs_api
      - ec_scheduler
      - minio
      - mongo-db
      - spark
    links:
      - "cache"
      - "ec_fs_api"
      - "ec_scheduler"
      - "minio"
      - "mongo-db"
      - "spark"
    restart: always
    ports:
      - "3309:3309"
    networks:
      - ec_network

  ec_scheduler:
    container_name: ec_scheduler
    image: embraconnect/ec_scheduler:latest
    build: .
    environment:
      - EC_FS_ENDPOINT=${EC_FS_ENDPOINT}
      - EC_FS_BUCKET=${EC_FS_BUCKET}
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - DELTA_LAKE_ENDPOINT=${DELTA_LAKE_ENDPOINT}
      - DELTA_LAKE_BUCKET=${DELTA_LAKE_BUCKET}
      - DELTA_LAKE_PATH=${DELTA_LAKE_PATH}
      - SPARK_WAREHOUSE_DIR=${SPARK_WAREHOUSE_DIR}
      - SPARK_ENDPOINT=${SPARK_ENDPOINT}
      - REDIS_HOST=${REDIS_HOST}
    depends_on:
      - cache
      - minio
      - mongo-db
      - spark
    links:
      - "cache"
      - "minio"
      - "mongo-db"
      - "spark"
    restart: always
    ports:
      - "8000:8000"
    networks:
      - ec_network

  cache:
    image: redis
    container_name: ec_broker
    restart: always
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
    ports:
      - "6379:6379"
    networks:
      - ec_network
    volumes:
      - ${PWD}/volumes/redis_data:/data

  minio:
    image: quay.io/minio/minio:latest
    container_name: ec_object_store
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    ports:
      - "9000:9000" # MinIO API
      - "9001:9001" # MinIO Console
    networks:
      - ec_network
    volumes:
      - ${PWD}/volumes/minio_data:/data
    user: "${UID}:${GID}" # Runs as the current user
    command: server /data --console-address ":9001"
    restart: unless-stopped

  mongo-db:
    image: mongo:latest
    container_name: ec_data_store
    restart: unless-stopped
    ports:
      - "27017:27017"
    networks:
      - ec_network
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_INITDB_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD}
      MONGO_INITDB_DATABASE: ${MONGO_INITDB_DATABASE}
    volumes:
      - ./mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js
      - ${PWD}/volumes/mongo_data:/data/db

  spark:
    image: "apache/spark:3.5.3-scala2.12-java11-r-ubuntu"
    container_name: ec_spark
    restart: unless-stopped
    command: >
      /opt/spark/sbin/start-connect-server.sh
      --packages "org.apache.spark:spark-connect_2.12:3.5.3,io.delta:delta-spark_2.12:3.0.0,org.apache.hadoop:hadoop-aws:3.3.4,software.amazon.awssdk:s3:2.20.25,software.amazon.awssdk:sts:2.20.25"
      --conf "spark.driver.extraJavaOptions=-Divy.cache.dir=/tmp -Divy.home=/tmp"
      --conf "spark.hadoop.fs.s3a.access.key=${MINIO_ROOT_USER}"
      --conf "spark.hadoop.fs.s3a.secret.key=${MINIO_ROOT_PASSWORD}"
      --conf "spark.hadoop.fs.s3a.endpoint=${SPARK_WAREHOUSE_ENDPOINT}"
      --conf "spark.hadoop.fs.s3a.path.style.access=true"
      --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem"
      --conf "spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
      --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension"
      --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"
    environment:
      - SPARK_NO_DAEMONIZE=true
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - SPARK_WAREHOUSE_ENDPOINT=${SPARK_WAREHOUSE_ENDPOINT}
    ports:
      - "4040:4040"     # Spark UI
      - "15002:15002"   # Spark Connect Server port
    networks:
      - ec_network

networks:
  ec_network:
    driver: bridge

volumes:
  redis_data:
    driver: local